---
phase: 03-benchmarks-validation
plan: 02
type: execute
---

<objective>
Create performance benchmark harness measuring boot time, memory footprint, BPF load time, and interrupt-to-BPF latency. Generate comparison report template for Linux benchmarking.

Purpose: Quantitative validation of the Axiom thesis — numbers that prove the kernel is small, fast, and efficient compared to Linux. These metrics are essential for the academic paper and investor pitch.
Output: Benchmark program with timing instrumentation, benchmark results from QEMU, comparison methodology document.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-benchmarks-validation/03-01-SUMMARY.md

**Key files:**
@kernel/src/lib.rs
@kernel/src/time.rs
@kernel/src/bpf/mod.rs
@kernel/src/bpf/helpers.rs
@kernel/src/syscall/bpf.rs
@userspace/minilib/src/lib.rs
@userspace/bpf_loader/src/main.rs
@docs/proposal.md

**Target metrics (from proposal.md):**
| Metric | Target | Stretch |
|--------|--------|---------|
| Kernel memory footprint | <10MB | <5MB |
| Boot to init | <1s | <500ms |
| BPF load time | <10ms | <1ms |
| Interrupt latency | <10μs | <1μs |

**Available timing infrastructure:**
- bpf_ktime_get_ns() helper — nanosecond kernel time
- kernel/src/time.rs — boot time tracking
- minilib::clock_gettime — userspace time access
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create benchmark harness with timing instrumentation</name>
  <files>userspace/bpf_loader/src/main.rs, kernel/src/lib.rs</files>
  <action>
  Create a benchmark program (can reuse or extend bpf_loader, or create new userspace/benchmark/) that measures:

  **1. Boot time (kernel-side instrumentation):**
  - Read kernel/src/time.rs to understand existing boot time tracking
  - If not already measured: add timestamp at start of kernel_main and at init process spawn
  - Print "Boot time: [X] ms" on serial during boot
  - This is kernel-side only — add timing printfs to kernel/src/lib.rs init()

  **2. Memory footprint (kernel-side):**
  - After boot, print total physical memory used, kernel heap size, BPF subsystem memory
  - Read kernel/src/mem/ to understand frame allocator stats
  - Print "Kernel memory: [X] KB" on serial during boot

  **3. BPF program load time (userspace benchmark):**
  - In the benchmark program:
    a. Read time before sys_bpf(BPF_PROG_LOAD)
    b. Load a standard BPF program (the 27-instruction demo from Phase 1)
    c. Read time after sys_bpf returns
    d. Print "BPF load time: [X] μs"
    e. Repeat 10 times and show min/max/avg

  **4. Interrupt-to-BPF latency:**
  - Create a BPF program attached to timer that:
    a. Calls bpf_ktime_get_ns() to get current time
    b. Writes timestamp to ringbuf
  - Userspace compares successive timestamps to infer timer interrupt interval
  - Alternatively: add a timestamp BEFORE execute_hooks in the timer handler, and have the BPF program read ktime — the difference is the dispatch latency
  - Print "Timer interrupt interval: [X] μs avg"

  **5. Print summary table:**
  ```
  === Axiom Benchmark Results ===
  Boot to init:        [X] ms
  Kernel memory:       [X] KB
  BPF load time:       [X] μs (avg of 10)
  Timer interval:      [X] μs (avg of 100)
  ==============================
  ```

  Focus on what's measurable from QEMU. Hardware-specific numbers (real interrupt latency) need RPi5.

  Avoid: Don't try to benchmark Linux — that's manual work for the user. Just measure Axiom's numbers.
  </action>
  <verify>cargo build succeeds, benchmark program compiles</verify>
  <done>Benchmark harness measures boot time, memory footprint, BPF load time, and timer interval</done>
</task>

<task type="auto">
  <name>Task 2: Run benchmarks on QEMU and create comparison report template</name>
  <files>docs/benchmarks.md</files>
  <action>
  1. Configure init to run the benchmark program
  2. Build and run on QEMU (x86_64)
  3. Capture benchmark results from serial output

  4. Create docs/benchmarks.md with:
     a. **Axiom results section** — fill in with actual QEMU numbers
     b. **Linux comparison methodology:**
        - How to set up minimal Buildroot Linux for RPi5
        - What to measure and how (time from power-on to userspace, /proc/meminfo, eBPF load time with bpftrace)
        - Fair comparison criteria (same hardware, same workload, same measurement points)
     c. **Comparison table template:**
        | Metric | Axiom | Linux | Ratio |
        |--------|-------|-------|-------|
        | Boot time | [measured] | [user fills in] | [calc] |
        | Memory footprint | [measured] | [user fills in] | [calc] |
        | BPF load time | [measured] | [user fills in] | [calc] |
        | Interrupt latency | [measured] | [user fills in] | [calc] |
     d. **Notes section** explaining QEMU vs hardware differences
     e. **Proposal alignment** — compare actual results against targets from docs/proposal.md

  5. Verify results look reasonable (boot < 1s, memory < 10MB, BPF load < 10ms)

  Avoid: Don't fabricate numbers — use actual QEMU measurements. Note which metrics need hardware for accurate numbers.
  </action>
  <verify>docs/benchmarks.md exists with actual Axiom QEMU numbers and Linux comparison methodology</verify>
  <done>Benchmark report with Axiom numbers from QEMU, comparison table template for Linux, methodology documentation</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Benchmark harness compiles and runs
- [ ] Boot time, memory, BPF load time, timer interval all measured
- [ ] Results printed in summary table on serial
- [ ] docs/benchmarks.md created with actual numbers
- [ ] Linux comparison methodology documented
- [ ] Results align with proposal targets (or deviations explained)
</verification>

<success_criteria>

- All 4 benchmark metrics measured on QEMU
- Results documented in docs/benchmarks.md
- Linux comparison methodology clearly documented for user to execute
- Numbers validate (or reality-check) proposal claims
- Phase 3 complete
</success_criteria>

<output>
After completion, create `.planning/phases/03-benchmarks-validation/03-02-SUMMARY.md`:

Phase 3 complete. Ready for Phase 4: Docs & Ecosystem.
</output>
